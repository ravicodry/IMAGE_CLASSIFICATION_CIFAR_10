# -*- coding: utf-8 -*-
"""Machine Learning Course Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lAQD-ulF1GMg7SRhJgcppQ694h7N6IsC

# **Import Modules**
"""

# Commented out IPython magic to ensure Python compatibility.
import time #using time function
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline
np.random.seed(2017) 
from tensorflow import keras
from keras.models import Sequential #help in creating nueral  network
from keras.layers.convolutional import Convolution2D, MaxPooling2D# use in cnn
from keras.layers import Activation, Flatten, Dense, Dropout# use in cnn
from keras.layers.normalization import BatchNormalization# use in learning parameter.
from keras.utils import np_utils
from tensorflow.keras import datasets, layers, models# import dataset
import tensorflow as tf# tensorflow function implementing neural network
from tensorflow import keras # import keras library
import matplotlib.pyplot as plt # use in ploting
!pip install keras

"""# **Load CIFAR10 Dataset and Data Exploration**"""

from keras.datasets import cifar10
(train_features, train_labels), (test_features, test_labels) = cifar10.load_data()#divide dataset into train and test feature
num_train, img_channels, img_rows, img_cols =  train_features.shape
num_test, _, _, _ =  test_features.shape
num_classes = len(np.unique(train_labels))# count the number of classes.

from keras.datasets import cifar10
(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()

print('---------------------------------')
print('X-Train Shape:', X_train.shape)
print('y-Train Shape:', y_train.shape)
print('---------------------------------')
print('X-Test Shape: ', X_test.shape)
print('y-Test Shape: ', y_test.shape)
print('---------------------------------')

"""The above cell output shows that input has 50000 image having dimension 32-32-3
and test output have 10000 image which contain only type of class.
"""

y_train.shape

y_train[:5]

"""y_train is a 2D array, for our classification having 1D array is good enough, so we will convert this to now 1D array"""

y_train = y_train.reshape(-1,)
y_train[:5]

"""y_train is now a 1D array

"""

y_test = y_test.reshape(-1,)
y_test[:5]
y_test.shape

print('Final Shape of Datasets:')
print('---------------------------------')
print('X-Train Shape:', X_train.shape)
print('y-Train Shape:', y_train.shape)
print('---------------------------------')
print('X-Test Shape: ', X_test.shape)
print('y-Test Shape: ', y_test.shape)
print('---------------------------------')

"""# **Data Preparation and Normalization**
we need our output in terms of binary class so that we can match our desired output.

Normalization is neccesary to avoid effect of outliers.All the pixel value will lie in the range of 0 to 1 having zero mean.

We also need to tranform our image dimension to 1D so that We can use a list of pixel value as our input varibale.i.e 3072 input variable we have for a single image.
"""

#--- COVERTING CLASS LABELS TO BINARY CLASS LABELS
y_train = np_utils.to_categorical(y_train, num_classes = 10)
y_test = np_utils.to_categorical(y_test, num_classes = 10)

#--- TRANSFORM IMAGES FROM (32, 32, 3) TO 3072-DIMENSIONAL VECTORS (32*32*3)
X_train = np.reshape(X_train,(50000,3072))
X_test = np.reshape(X_test,(10000,3072))

#--- NORMALIZATION OF PIXEL VALUES (TO [0 - 1] RANGE)
X_train = X_train.astype('float32')/255
X_test = X_test.astype('float32')/255

"""# **[a]- MLP (MultiLayer Perceptron) Classifier**

MLP is the most basic neural network after Mp neuron.
Any function can be implemnetd using MLP.
"""

# import required library for generating neural network.
from keras.models import Sequential 
from keras.layers import Dense, Activation
from keras.optimizers import SGD

model = Sequential()# Neural network object created
model.add(Dense(256, activation='relu', input_dim=3072))# we are using 3 hidden layer in this MLP model.
model.add(Dense(256, activation='relu'))# Relu function used in between hidden layer.
model.add(Dense(10, activation='softmax'))# since our output is in multi binary class,therefore we have to use softmax activation function.
sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)# define learning algorithm and its parameter, it is the most important step in neural network
#because it only update weights.

model.compile(optimizer=sgd,
              loss='categorical_crossentropy', # we are using cross entropy objective function.
              metrics=['accuracy'])

"""## Model description:"""

model.summary

"""# Training the MLP-
Let's train our model now! We will store the training loss values and metrics in a history object, so we can visualize the training process later. We are going to train the model for 15 epochs, using a batch size of 32 and a validation split of 0.2. The latter means that 20% of our training data will be used as validation samples (in practice however it is advised to separate the validation data from the training data altogether).
"""

history = model.fit(X_train,y_train, epochs=15, batch_size=32, verbose=2, validation_split=0.2)

"""## Plot the losses
Train loss vs validation losses
"""

def plotLosses(history):  
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model - Loss')
    plt.ylabel('loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()

plotLosses(history)

"""Observation:

Both training and validation loss decrease as the number of epoch increase.

Train loss sharply decarse after two epoches but validtion loss decrease slowly.

After 14 epoch validation loss become almost stagnent.

# Evaluating the MLP
"""

score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)
print(model.metrics_names)
print(score)

"""After implementing MLP we Observe accuracy 47% which is very low. 

so we will try different model and see wether accuarcy can be increased.

# **[b]- CNN (Convolutional Neural Network) Classifier**

# Dataset Preparation
"""

!pip install np_utils

from tensorflow.keras.utils import to_categorical
#from keras.utils import to_categorical
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
y_train = to_categorical(y_train, num_classes=10)# convert dta into categorical type
y_test = to_categorical(y_test, num_classes=10)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

print("Shape of training data:")
print(X_train.shape)
print(y_train.shape)
print("Shape of test data:")
print(X_test.shape)
print(y_test.shape)

"""# Creating CNN Model"""

from keras.layers import Dense, Flatten
from keras.layers import Conv2D, MaxPooling2D

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(10, activation='softmax'))

sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=sgd)
model.summary()

"""##Model description:"""

model.summary

"""# Training the CNN-"""

history = model.fit(X_train, y_train, batch_size=32, epochs=15, verbose=2, validation_split=0.2)

"""## Plot the losses
Train loss vs validation losses
"""

plotLosses(history)

"""Observation:

-Training error is steady and very low, it is not changing with the number of epoch.

-Validation error increase as the epoch increasing.

# Evaluating the CNN
"""

score1 = model.evaluate(X_test, y_test, batch_size=128, verbose=0)

print(model.metrics_names)
print(score1)

"""After implementing MLP we Observe accuracy 64% which is better than MLP model 

Still we will try different model and see wether accuarcy can be increased.

# **[c]- TensorFlow**

We will do some changes in the model and try to implement again.
"""

import tensorflow as tf   
  
# Display the version
print(tf.__version__)

from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout
from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Model

(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

# Reduce pixel values

X_train, X_test = X_train / 255.0, X_test / 255.0
  
# flatten the label values
y_train, y_test = y_train.flatten(), y_test.flatten()

# visualize data by plotting images
fig, ax = plt.subplots(5, 5)

k = 0
  
for i in range(5):
    for j in range(5):
        ax[i][j].imshow(X_train[k], aspect='auto')
        k += 1
  
plt.show()

# number of classes
K = len(set(y_train))
  
# calcutate total numer of clases 
# for output layer
print("number of classes:", K)

# Build the model using the functional API
# input layer
i = Input(shape=X_train[0].shape)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(i)
x = BatchNormalization()(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2, 2))(x)

x = Flatten()(x)
x = Dropout(0.2)(x)

# Hidden layer
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)

# last hidden layer i.e.. output layer
x = Dense(K, activation='softmax')(x)

model = Model(i, x)

# model description
model.summary()

# Compile
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Fit
r = model.fit(
  X_train, y_train, validation_data=(X_test, y_test), epochs=50)

# Fit with data augmentation
# Note: if you run this AFTER calling
# the previous model.fit()
# it will CONTINUE training where it left off
batch_size = 32
data_generator = tf.keras.preprocessing.image.ImageDataGenerator(
  width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
  
train_generator = data_generator.flow(X_train, y_train, batch_size)
steps_per_epoch = X_train.shape[0] // batch_size
  
r = model.fit(train_generator, validation_data=(X_test, y_test),
              steps_per_epoch=steps_per_epoch, epochs=50)

# Plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc', color='red')
plt.plot(r.history['val_accuracy'], label='val_acc', color='green')
plt.legend()

"""OBSERVATION:

-Validation accuracy and Accuracy increase on every epoch.

## TESTING ON SINGLE IMAGE INPUT
"""

# label mapping
  
labels = '''airplane automobile bird cat deerdog frog horseship truck'''.split()
  
# select the image from our test dataset
image_number = 0
  
# display the image
plt.imshow(X_test[image_number])
  
# load the image in an array
n = np.array(X_test[image_number])
  
# reshape it
p = n.reshape(1, 32, 32, 3)
  
# pass in the network for prediction and 
# save the predicted label
predicted_label = labels[model.predict(p).argmax()]
  
# load the original label
original_label = labels[y_test[image_number]]
  
# display the result
print("Originaxl label is {} and predicted label is {}".format(
    original_label, predicted_label))

# save the model
model.save('Machine Learning Course Project.h5')

"""# **[d]- Logistic Regression**

*   List item
*   List item


First, we load our CIFAR-10 dataset.
Then, we upload images in batches of 100 at a time.
Then, we convert our images into 3x32x32 tensors. This makes our batch matrix 100 x 3 x 32 x 32 in dimensions.
Then, we pass each batch into the CIFAR10 model, which converts the tensors from 100 x 3 x 32 x 32 to 100 x 3072.
Then, we multiply it by the weights matrix, which is [10 x 3072], which is transposed to be multiplied by each tensor batch
This multiplication gives a 100 x 10 results matrix, with logits, and we add the bias.
Then, softmax is applied on the logits to convert them into probabilities
Then, cross entropy is performed on the probabilites matrix to give a continuous and differentiable cost function.
Then, cross entropy from all the different batches is combined to give a total training loss.
Then we calculate the partial derivatives of this cross entropy function with respect to every one of the 10x3072 weights and biases.
Then, we subtract the gradient (the vector of all 10x3072 partial derivatives) from the vector of current thetas, for each batch. (so, youre subtracting from theta 100 times)
We do step 11 for each number of epochs.
"""

import torch
from pathlib import Path
import pandas as pd
import io
import matplotlib.pyplot as plt
import numpy as np
import torchvision
import torchvision.transforms as transforms

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True)
testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                        download=True)
# labels 
classes = ('airplane', 'automobile', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
print('-----------------------------')
print('length of training set: ' + str(len(trainset)))
print('length of test set: ' + str(len(testset)))
print('-----------------------------')

trainset[0]

example = 0
image, label = trainset[example] #since each element of the trainset list is itself a tuple with the image details, and then the label
print("this is an image of a " + classes[(trainset[example])[1]]) # first index into tuple in trainset, then the 2nd value (label), and then the classes
plt.imshow(image)

"""# Transforming images into matrices of color values"""

import torchvision.transforms as transforms

datasetT = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transforms.ToTensor())

example = 5
imgTensor, label = datasetT[example]
print('size of image matrix: ' + str(imgTensor.shape))
print("this is an image of a " + classes[(trainset[example])[1]])

print(imgTensor[:, 0:2, 0:2])

example = 21344
imgTensor, label = datasetT[example]
plt.imshow(imgTensor[1, 0:32, 0:32], cmap='Greens')
# change the 0 to 1 or 2 for different color channels.
print("R channel of image 0 in dataset")

"""# Splitting up dataset into Training and Validation sets"""

#length of training examples.
m = 50000 

#percentage of m dedicated to CV.  
pCV = 0.2

# give the amount of examples dedicated to CV. 
mCV = int(m*pCV)
print("amount of training examples: " + str(m - mCV))
print("amount of cross validation examples: " + str(mCV))

def splitIndices(m, pCV):
  """ randomly shuffle a training set's indices, then split the indices into training and cross validation sets.
   Pass in 'm', length of training set, and 'pCV', the percentage of the training set you would like 
   to dedicate to cross validation."""
   
  # determine size of CV set.
  mCV = int(m*pCV)

  #create random permutation of 0 to m-1 - randomly shuffle all values from 0 to m.
  indices = np.random.permutation(m)

  #pick first mCV indices for training, and then validation.
  return indices[mCV:], indices[:mCV]

trainIndices, valIndices = splitIndices(m, pCV)

# print the first few indices of the training set, and first few of the validation set, as a sanity check to see that they are shuffled.
print("length of training set: " + str(len(trainIndices)))
print("length of cross validation set: " + str(len(valIndices)))
print()
print('sample validation indices: ' + str(list(valIndices[0:8])))
print('sample educational indices: ' + str(list(trainIndices[0:8])))

"""# The below code does the same as the above."""

from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data.dataloader import DataLoader

batchSize = 100

# TRAIN SET

# training sampler and data loader - creates a SubsetRandomSampler object that takes random samples of the numbers in trainIndices, or random indices.
trainSampler = SubsetRandomSampler(trainIndices)


# training loader - creates a dataloader object which takes the indices from trainSampler, 
# and when given batchSize, takes random batches of batchSize from the indices list, and then pairs it with
# the respective dataset in datasetT
trainLoader = DataLoader(datasetT, batchSize, sampler=trainSampler)

print(list(trainLoader))

# VALIDATION SET

valSampler = SubsetRandomSampler(valIndices)
valLoader = DataLoader(datasetT, batchSize, sampler=valSampler)

"""# LOGISTIC REGRESSION MODEL"""

import torch.nn as nn

# this will dictate the rows of the theta matrix
inputSize = 3*32*32

# this will dictate the columns of the theta matrix
numClasses = 10

# create our linear regression model (nn.Linear creates bias terms for us)
model = nn.Linear(inputSize, numClasses)

print('dimensions of weight matrix: ' + str(model.weight.shape))
print(model.weight)

# BELOW LIST OF LEARN WEIGHT

#PRINT THE LABEL WITH CORRESPONDING IMAGES.
for images, labels in trainLoader: 
   print(labels)

class CIFAR10(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(inputSize, numClasses)
        
    def forward(self, xb):
        xb = xb.reshape(-1, 3072)
        out = self.linear(xb)
        return out
    
model = CIFAR10()

print(model.linear.weight.shape)
print(model.linear.bias.shape)

for images, labels in trainLoader:
    outputs = model(images)
    break

print('outputs.shape :', outputs.shape)
print('sample outputs :\n', outputs[:2]) # print 2 out of the 100 rows of the total output vector

import torch.nn.functional as F

# CONVERT ABOVE OUTPUT INTO PROBABILITIES
# apply the softmax for each output row in our 100 x 10 output (with batch size 100)
probs = F.softmax(outputs, dim=1)

# look at some sample probabilities
print("sample probabilities:\n", probs[:2].data)
# add up the probabilities of each row for a sanity check that they equal 1 now
print(sum(list(probs[0])))

maxProbs, preds = torch.max(probs, dim=1) #torch.max returns the max value itself (maxProbs) as well as the index of the prediction (preds)
print(preds)
print(maxProbs)

"""# Evaluation and Cost function"""

labels==preds

def accuracy(preds, labels):
  return torch.sum(labels==preds).item() / len(labels)

"""EVALUATION OF LOGISTIC REGRESSION:"""

accuracy(preds, labels)

lossFn = F.cross_entropy
# loss for current batch of data
# remember that outputs are our thetas gathered from the model
# labels is correct answers from dataLoader
loss = lossFn(outputs, labels)
print(loss)

"""Since loss and accurcay are very bad. we need some hyperparameter tuning

# Optimization using SDG (stochastic gradient descent)
"""

learningRate = 0.001
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)

"""# Training the model"""

# recall that xb is the X (a list batchSize long of 3x32x32 images) for a batch. yb is the corresponding labels for those images.

def lossBatch(model, lossFn, xb, yb, opt=None, metric=None):
  # calculate the loss
  preds = model(xb)
  loss = lossFn(preds, yb)

  if opt is not None:
    # compute gradients
    loss.backward()
    # update parameters
    opt.step()
    # reset gradients to 0 (don't want to calculate second derivatives!)
    opt.zero_grad()

  metricResult = None
  if metric is not None:
    metricResult = metric(preds, yb)

  return loss.item(), len(xb),  metricResult

def evaluate(model, lossFn, validDL, metric=None):
  #with torch.no_grad (this was causing an error)
  
  # pass each batch of the validation set through the model to form a multidimensional list (holding loss, length and metric for each batch)
  # the reason why we made optimization optional is so we can reuse the function here
  results = [lossBatch(model, lossFn, xb, yb, metric=metric,) for xb,yb in validDL]

  # separate losses, counts and metrics
  losses, nums, metrics = zip(*results)

  # total size of the dataset (we keep track of lengths of batches since dataset might not be perfectly divisible by batch size)
  total = np.sum(nums)

  # find average total loss over all batches in validation (remember these are all vectors doing element wise operations.)
  avgLoss = np.sum(np.multiply(losses, nums))/total

  # if there is a metric passed, compute the average metric
  if metric is not None:
    # avg of metric accross batches
    avgMetric = np.sum(np.multiply(metrics, nums)) / total

  return avgLoss, total, avgMetric

def accuracy(outputs, labels):
  _, preds = torch.max(outputs, dim=1) # underscore discards the max value itself, we don't care about that
  return torch.sum(preds == labels).item() / len(preds)

E = evaluate(model, lossFn, valLoader, metric=accuracy)

print("training set loss: ", loss)
print("cross validation set loss: ", E[0])

def fit(epochs, model, lossFn, opt, trainDL, valDL, metric=None):
  valList = [0.10]
  for epoch in range(epochs):
    # training - perform one step gradient descent on each batch, then moves on
    for xb, yb in trainDL: 
      loss,_,lossMetric = lossBatch(model, lossFn, xb, yb, opt)
      

    # evaluation on cross val dataset - after updating over all batches, technically one epoch
    # evaluates over all validation batches and then calculates average val loss, as well as the metric (accuracy)
    valResult = evaluate(model, lossFn, valDL, metric)
    valLoss, total, valMetric = valResult
    valList.append(valMetric)
    # print progress
    if metric is None: 
      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch + 1, epochs, valLoss))
    else:
      print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch + 1, epochs, valLoss, metric.__name__, valMetric))

  return valList

# redefine model and optimizer
learningRate = 0.009
model = CIFAR10()
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)

trainList = fit(100, model, lossFn, optimizer, trainLoader, valLoader, metric=accuracy)

test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())

testLoader = DataLoader(test, batchSize)

avgLoss, total, avgMetric = evaluate(model, F.cross_entropy, testLoader, metric=accuracy)
print("test set accuracy: \n", avgMetric*100,"%")
avgLoss, total, avgMetric = evaluate(model, F.cross_entropy, valLoader, metric=accuracy)
print("cross validation set accuracy: \n",avgMetric*100,"%")
avgLoss, total, avgMetric = evaluate(model, F.cross_entropy, trainLoader, metric=accuracy)
print("training set accuracy: \n",avgMetric*100,"%")

"""After hyperparameter tuning Accuracy of logistic regression  has improved."""